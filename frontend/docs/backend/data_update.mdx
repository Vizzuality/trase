---
name: Database update process
menu: Backend
---

# Database update process

The database is updated with new datasets through an import process. The current implementation is referred to as "mirror import" in the code. It fetches new data from a database dump stored in S3 and copies it into schema TRASE_LOCAL_MIRROR_SCHEMA. Then it runs the process of updating the live tables in TRASE_LOCAL_SCHEMA, making a best attempt at preserving existing configuration tables (not part of the imported schema.)

Post-update, data is checked using a [validation script](data_validation.md).

There is also a utility to copy the database between instances using an [export / import](data_export_import.md) approach.

All of these tools are available via the admin panel.

## Starting the database update process

The importer can be started in two ways:
- via a rake task: `SCHEMA_VERSION=... bundle exec rake db:mirror:import`
- via the admin interface: `/content/admin/database_update`

In the first case the importer will run synchronously. In the latter case it will be executed as a background job using sidekiq. For that to work you need redis & sidekiq running. If redis is not already running it can be started using `redis-server`. To start sidekiq in development run `bundle exec sidekiq`. In staging / demo / production starting and stopping sidekiq is handled by capistrano.

## Differences between the source dump and Trase database

The differences between the two databases are as follows:
- the source dump contains the core data regarding flows, nodes and attributes;
- the Trase database contains the core data as well as additional tables which describe how to visualise that data.

There is overlap between the two databases, which is the core data to be displayed in Trase. These are the tables which hold that data:
- `countries`
- `commodities`
- `contexts`
- `node_types`
- `context_node_types`
- `download_versions`
- `inds`
- `quals`
- `quants`
- `nodes`
- `node_inds`
- `node_quals`
- `node_quants`
- `flows`
- `flow_inds`
- `flow_quals`
- `flow_quants`

They are referred to as "blue tables". The remaining tables in the Trase database, which are linked to the blue tables and describe how to visualise data, are referred to as "yellow tables".

For example, the blue table `inds` holds basic information about an attribute called `SOY_YIELD` and the yellow table `ind_properties` holds the information about what tooltip to display with that attribute.

The objective of the import process is to copy the blue tables verbatim into Trase DB (overwriting previous content), but to preserve any yellow tables information which remains relevant.

## Replacing core data while preserving matching configuration

The importer script code is contained in `lib/api/v3/import/importer.rb`. The script goes over tables in a specific order, whereby each table is preceded by all the tables it depends on. *Note to future self*: for this to continue working as the database grows, cycles in the schema must be avoided.

Tables are processed differently depending on whether they are "blue" (core) or "yellow" (configuration). It can be easily recognised which is which based on the subclass of the model: either `Api::V3::BlueTable` or `Api::V3::YellowTable`.

In the first step all the tables are backed up, which means different things in case of blue and yellow tables.

For the blue tables we store a mapping between the old `id` (from local table) and new `id` (from remote table). This is a temporary table with just two columns: `id` and `new_id`.

*Note to future self: the assumption here is that each of the blue tables has a unique numeric identifier called `id` in both the local and remote table. We assume those identifiers are not stable across data updates, which is why we create a mapping in this step.*

In order to match the local table to the remote table we use columns specified in `import_key`, which is a method that needs to be defined for each blue table. For example, the import key for the `countries` table contains only the `iso2` column (which has a unique constraint set) and so local and remote can be matched on this column. Sometimes the import key contains foreign keys, for example `contexts` has an `import_key` consisting of `country_id` and `commodity_id`; of course those are unstable foreign keys, so they cannot be used to match rows directly in the remote table; instead, we need to match using the id mapping of both `countries` and `commodities` tables. The script knows to do that based on the list of unstable foreign keys which are called `blue_foreign_keys` and detects if the `import_key` overlaps with `blue_foreign_keys`, requiring a lookup in the mapping. That mapping needs to be available at the time, which is why order of processing blue tables matters.

For the yellow tables, we simply back up the entire table in a temporary table.

Once all tables are backed up, the import can commence. The import process iterates over blue tables and their respective dependant yellow tables. Consider this chain of dependencies, which shows that yellow tables can also depend on each other and therefore the order in which yellow tables are processed is also relevant:

`contexts` (blue) -> `contextual_layers` (yellow) -> `carto_layers` (yellow)

For each blue table, truncate the local table and copy the remote table into it. Next, if there are any yellow tables that depend on this blue table, they need to be restored from backup. Restoring consists of the following steps:
- if there are any obsolete rows in the yellow tables backup, remove them first. To determine which rows are obsolete we match rows on foreign keys, which can be of two types: blue and yellow (depending on which parent table is concerned). The rule is, yellow foreign keys are stable (those ids do not change) and blue foreign keys are unstable. To match obsolete rows by blue foreign keys we need to do a lookup on the mapping table.
- if there are any blue foreign keys in the yellow table, they need to be resolved in the remaining rows by replacing them with new values from respective mapping tables
- finally, the yellow table with removed obsolete rows and resolved foreign keys can be restored from backup

To sum up, these are the important helper methods that need to be defined in models for the import process to run correctly:
- `import_key` - for blue tables only. Used for matching rows from local table with remote. Exception: `flows`. That table does not lend itself very well to matching in this way, which is why the `import_key` is empty. That results with the table being copied without any hope of resolving the old identifiers against the new ones. For now that is not a problem, because we do not have any yellow tables depending on flows. If we have any, this needs to be considered and matching on `context_id`, `year` and `path` implemented.
- `blue_foreign_keys` - for both blue and yellow tables. Used to inform the importer that identifiers needs to be resolved using the mapping tables.
- `yellow_foreign_keys` - for yellow tables only. Allows the importer to check for any rows that depend on another yellow table and might need removing.

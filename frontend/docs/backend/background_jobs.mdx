---
name: Background jobs
menu: Backend
---

# Background jobs

We use `sidekiq` to process long running jobs in the background.

We're using it for a number of database jobs (app/workers):
- importing indicators from World Bank APIs
- importing indicators from ComTrade API (currently defunct, due to old API being retired)
- exporting CMS configuration. That is part of a private API that was a first step towards automating CMS configuration.
- database export - export a dump of the database from the current environment to S3
- database import - import a dump from S3 into the current database
- flow attribute available years update - automation for calculating available years of data per each flow indicator
- full refresh - used to refresh a materialised view (used in the refresh routine of selected models)
- partial refresh - used to partially refresh a materialised table  (used in the refresh routine of selected models)
- map attributes export - exports values to CartoDB, runs during deployment
- mirror database update worker - allows to update the database from a core data dump, never used in production, mostly used in sandbox / demo to prepare data
- refresh actor basic attributes - refreshes the summary part of the profile for actor nodes
- precomputed download refresh worker - recomputes batch downloads
- upsert attributes worker - used for refreshing the attributes "materialised table"

Jobs are sorted into different sidekiq queues based on how long they take to run.

For some jobs we need to ensure only one database job can run at any single time. For that we use `sidekiq-unique-jobs` gem. It works at 2 stages:
- when adding a job to the queue,
- when pulling the job from the queue to start execution.

It can be configured to lock execution according to a number of predefined strategies and the one we use is called `:until_and_while_executing`. It works as follows: you can enqueue a duplicate job A' unless job A is already enqueued; A' will not start executing until A is finished, at which point it will be possible to enqueue another job A''. It is also important to understand that the unique locks have an expiration time, which I set to match average execution time. It seems that without this in place it is possible that an enqueued job will start executing too early (after a default timeout of 60 seconds).

In practice this means that if the admin makes 3 updates in a short time frame ( < lock expiration time) to e.g. Download Attributes, normally the `DownloadFlowsRefreshWorker` would be enqueued and executed 3 times, possibly concurrently. With the unique lock in place, it will enqueue the first one and if possible start executing immediately, possibly allowing to enqueue the second one while first one is in progress; the third one will never be enqueued. Once the first one has completed the second one will be executed.

Dead jobs might leave leftover locks behind. The consequence is that new jobs are not scheduled. Cleanup should be handled by the bit of config in the sidekiq serializer, as [described here](https://github.com/mhenrixon/sidekiq-unique-jobs#cleanup-dead-locks). When no jobs are running and yet new jobs are not scheduled, manual intervention may be required. In the console:
- `SidekiqUniqueJobs::Digests.all` - if this is not empty, then these are the digests of dead locks
- delete them like: `SidekiqUniqueJobs::Digests.del(digest: "uniquejobs:cf1d9fb118c1424fc25cbebdc953fd25")`
